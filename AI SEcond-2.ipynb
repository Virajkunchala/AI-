{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09c4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11da24be",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633229a7",
   "metadata": {},
   "source": [
    "TensorFlow is an open-sourced end-to-end platform, a library for multiple machine learning tasks, while Keras is a high-level neural network library that runs on top of TensorFlow. \n",
    "Both provide high-level APIs used for easily building and training models, \n",
    "but Keras is more user-friendly because it's built-in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d0740",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea8812",
   "metadata": {},
   "source": [
    "There are two types of models in Keras: the Sequential model and the Functional API.\n",
    "\n",
    "in The Sequential model you can add one layer at a time using the add() method. \n",
    "It is a simple way to create a model, and is suitable for most cases. \n",
    "The layers are added to the model in the order in which they are called, \n",
    "and the output of one layer is used as the input for the next layer. \n",
    "It is simple for creating feedforward neural network where layers are stacked up one after the other.\n",
    "\n",
    "Here is an example of creating a simple feedforward neural network using Sequential Model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6fa925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316dfb4f",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b666a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd337260",
   "metadata": {},
   "source": [
    "In deep learning, the loss function is used to measure the difference between the predicted output and the actual output for a given input.\n",
    "The goal of training a model is to minimize this difference, or loss, by adjusting the model's parameters.\n",
    "\n",
    "There are several types of loss functions commonly used in deep learning, each with their own strengths and weaknesses. \n",
    "Some common loss functions include:\n",
    "\n",
    "Mean Squared Error (MSE): This is a commonly used loss function for regression problems. \n",
    "It calculates the average of the squared differences between the predicted output and the actual output.\n",
    "\n",
    "Categorical Cross-Entropy: This is a commonly used loss function for classification problems with more than two classes.\n",
    "It measures the dissimilarity between predicted probability distribution and the true distribution.\n",
    "\n",
    "Binary Cross-Entropy: This is similar to categorical cross-entropy, \n",
    "but is used for binary classification problems. It measures the dissimilarity between predicted probability and true label.\n",
    "\n",
    "As the model is trained, the weights and biases of the model are adjusted in order to minimize the loss. \n",
    "This process is known as backpropagation. \n",
    "The model will iteratively adjust the weights and biases until the loss is minimized, and the model's predictions are as accurate as possible.\n",
    "\n",
    "In summary, the Loss function is a measure of the difference between the predicted output of a model and the actual output,\n",
    "and the objective of the training process is to minimize the loss. The model learns by iteratively adjusting the weights and biases to minimize the loss until the predictions are as accurate as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed5f96",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8aef21",
   "metadata": {},
   "source": [
    "Overfitting occurs when a model is trained too well on the training data, and as a result, it performs poorly on unseen data. \n",
    "This happens because the model has learned the noise in the training data, and as a result, \n",
    "it is not able to generalize well to new data. This can happen  when there is not enough training data.\n",
    "\n",
    "Underfitting occurs when a model is not able to capture the underlying patterns in the training data, \n",
    "and as a result, it performs poorly on both the training and unseen data. This happens  when there is not enough training data.\n",
    "\n",
    "There are several ways to identify and measure overfitting. One of the most common ways is to use a technique called cross-validation.\n",
    "Cross-validation involves splitting the available data into a training set and a validation set.\n",
    "The model is trained on the training set, and its performance is evaluated on the validation set. \n",
    "If the performance on the validation set is worse than on the training set, it is an indication of overfitting.\n",
    "Another way is to observe the performance of the model on unseen test data.\n",
    "If the model performs well on the training data but poorly on the test data, it is an indication of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc173243",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7178be12",
   "metadata": {},
   "source": [
    "The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model's internal parameters are updated\n",
    "\n",
    "    a batch size is the number of samples that are processed by the model in one forward and backward pass. The batch size is used to control the trade-off between the speed of training and the memory requirements of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dcf48f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 5us/step\n",
      "40960/29515 [=========================================] - 0s 4us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 4s 0us/step\n",
      "26435584/26421880 [==============================] - 4s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 2s 0us/step\n",
      "4431872/4422102 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)),\n",
       " (array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26eb62dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.9690 - accuracy: 0.6593\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5018 - accuracy: 0.8282\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4078 - accuracy: 0.8618\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3645 - accuracy: 0.8746\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3369 - accuracy: 0.8824\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3176 - accuracy: 0.8887\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2997 - accuracy: 0.8939\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2875 - accuracy: 0.8984\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2751 - accuracy: 0.9012\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2640 - accuracy: 0.9049\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3613 - accuracy: 0.8824\n",
      "Test accuracy: 0.8823999762535095\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one hot encoding\n",
    "y_train_enc = to_categorical(y_train)\n",
    "y_test_enc = to_categorical(y_test)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(4, activation='relu'))\n",
    "# even after 4 th hidden layer there is not much difference\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_enc, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_enc)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d48b94b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Hidden layers for the above dataset,i tried to add more hidden layers but i observed that there is not much difference\n"
     ]
    }
   ],
   "source": [
    "# 4a\n",
    "print(\"3 Hidden layers for the above dataset,i tried to add more hidden layers but i observed that there is not much difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151e0e8",
   "metadata": {},
   "source": [
    "# 4b\n",
    "The number of neurons per layer that work best for a model can depend on several factors, such as the complexity of the task, \n",
    "the amount of data available, and the architecture of the model. For the Fashion MNIST dataset, the example I provided uses 256 neurons in the first hidden layer, 128 neurons in the second hidden layer, and 64 neurons in the third hidden layer.\n",
    "\n",
    "It is generally a good practice to start with a small number of neurons and gradually increase the number of neurons as you add more layers. This allows the model to learn simple features in the early layers, and more complex features in the later layers.\n",
    "\n",
    "Having a large number of neurons in the layers may lead to overfitting, especially if the model has a lot of parameters and not enough data to learn from. On the other hand, having too few neurons might not allow the model to learn the necessary features, leading to underfitting.\n",
    "\n",
    "Another important factor to consider is the input size. For example, in our case the input size is 784 because we are flattening the 28x28 image. So the number of neurons in the first layer should be greater than 784."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c490fe1",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52f5cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 11s 3ms/step - loss: 0.1350 - accuracy: 0.9550 - val_loss: 0.6579 - val_accuracy: 0.8906\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 11s 3ms/step - loss: 0.0962 - accuracy: 0.9660 - val_loss: 0.6490 - val_accuracy: 0.8844\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 10s 3ms/step - loss: 0.0957 - accuracy: 0.9656 - val_loss: 0.6308 - val_accuracy: 0.8901\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 10s 3ms/step - loss: 0.0874 - accuracy: 0.9683 - val_loss: 0.6493 - val_accuracy: 0.8920\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 10s 3ms/step - loss: 0.0896 - accuracy: 0.9684 - val_loss: 0.6182 - val_accuracy: 0.8928\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 10s 3ms/step - loss: 0.0893 - accuracy: 0.9667 - val_loss: 0.6462 - val_accuracy: 0.8871\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 10s 3ms/step - loss: 0.0895 - accuracy: 0.9676 - val_loss: 0.6048 - val_accuracy: 0.8874\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 10s 3ms/step - loss: 0.0858 - accuracy: 0.9689 - val_loss: 0.6087 - val_accuracy: 0.8883\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 10s 3ms/step - loss: 0.0858 - accuracy: 0.9695 - val_loss: 0.6388 - val_accuracy: 0.8910\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 10s 3ms/step - loss: 0.0846 - accuracy: 0.9692 - val_loss: 0.5842 - val_accuracy: 0.8903\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0504 - accuracy: 0.9814 - val_loss: 0.7103 - val_accuracy: 0.8945\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0593 - accuracy: 0.9795 - val_loss: 0.7227 - val_accuracy: 0.8907\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0540 - accuracy: 0.9799 - val_loss: 0.7069 - val_accuracy: 0.8861\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0575 - accuracy: 0.9794 - val_loss: 0.7117 - val_accuracy: 0.8906\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0550 - accuracy: 0.9798 - val_loss: 0.7632 - val_accuracy: 0.8890\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0581 - accuracy: 0.9794 - val_loss: 0.7485 - val_accuracy: 0.8909\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0509 - accuracy: 0.9820 - val_loss: 0.7783 - val_accuracy: 0.8900\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0530 - accuracy: 0.9805 - val_loss: 0.7952 - val_accuracy: 0.8920\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0530 - accuracy: 0.9807 - val_loss: 0.7274 - val_accuracy: 0.8908\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0507 - accuracy: 0.9814 - val_loss: 0.8393 - val_accuracy: 0.8911\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0263 - accuracy: 0.9908 - val_loss: 0.8638 - val_accuracy: 0.8956\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0311 - accuracy: 0.9890 - val_loss: 0.8706 - val_accuracy: 0.8975\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0298 - accuracy: 0.9889 - val_loss: 0.8936 - val_accuracy: 0.8958\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0334 - accuracy: 0.9880 - val_loss: 0.9338 - val_accuracy: 0.8877\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0319 - accuracy: 0.9882 - val_loss: 0.9536 - val_accuracy: 0.8948\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0329 - accuracy: 0.9889 - val_loss: 0.9741 - val_accuracy: 0.8958\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0314 - accuracy: 0.9885 - val_loss: 0.9771 - val_accuracy: 0.8937\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0300 - accuracy: 0.9892 - val_loss: 0.8816 - val_accuracy: 0.8902\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0353 - accuracy: 0.9874 - val_loss: 0.9764 - val_accuracy: 0.8945\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 0.0264 - accuracy: 0.9904 - val_loss: 1.0308 - val_accuracy: 0.8939\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0144 - accuracy: 0.9952 - val_loss: 1.0766 - val_accuracy: 0.8945\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0161 - accuracy: 0.9947 - val_loss: 1.0913 - val_accuracy: 0.8970\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0124 - accuracy: 0.9956 - val_loss: 1.1000 - val_accuracy: 0.8911\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0157 - accuracy: 0.9943 - val_loss: 1.1607 - val_accuracy: 0.8919\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0193 - accuracy: 0.9926 - val_loss: 1.2003 - val_accuracy: 0.8889\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0153 - accuracy: 0.9944 - val_loss: 1.2805 - val_accuracy: 0.8900\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0211 - accuracy: 0.9924 - val_loss: 1.2369 - val_accuracy: 0.8919\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0148 - accuracy: 0.9948 - val_loss: 1.1923 - val_accuracy: 0.8910\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0214 - accuracy: 0.9921 - val_loss: 1.1697 - val_accuracy: 0.8902\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0169 - accuracy: 0.9941 - val_loss: 1.2390 - val_accuracy: 0.8895\n",
      "batch size: 16\n",
      "accuracy: 0.9692 - val_accuracy: 0.8903 - loss: 0.0846 - val_loss: 0.5842\n",
      "batch size: 32\n",
      "accuracy: 0.9814 - val_accuracy: 0.8911 - loss: 0.0507 - val_loss: 0.8393\n",
      "batch size: 64\n",
      "accuracy: 0.9904 - val_accuracy: 0.8939 - loss: 0.0264 - val_loss: 1.0308\n",
      "batch size: 128\n",
      "accuracy: 0.9941 - val_accuracy: 0.8895 - loss: 0.0169 - val_loss: 1.2390\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Load the dataset\n",
    "# (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# # Preprocess the data\n",
    "# X_train = X_train.reshape(60000, 784)\n",
    "# X_test = X_test.reshape(10000, 784)\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "# X_train /= 255\n",
    "# X_test /= 255\n",
    "\n",
    "# # one hot encoding\n",
    "# y_train_enc = to_categorical(y_train)\n",
    "# y_test_enc = to_categorical(y_test)\n",
    "\n",
    "# # Create the model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "results = []\n",
    "for bs in batch_sizes:\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train_enc, epochs=10, batch_size=bs, validation_data=(X_test, y_test_enc))\n",
    "    results.append(history.history)\n",
    "\n",
    "# print the results\n",
    "for i, bs in enumerate(batch_sizes):\n",
    "    print(f'batch size: {bs}')\n",
    "    print(f'accuracy: {results[i][\"accuracy\"][-1]:.4f} - val_accuracy: {results[i][\"val_accuracy\"][-1]:.4f} - loss: {results[i][\"loss\"][-1]:.4f} - val_loss: {results[i][\"val_loss\"][-1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8db2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
