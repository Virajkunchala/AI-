{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408a403a",
   "metadata": {},
   "source": [
    "# 1\n",
    "Flattening is a technique used to convert a multi-dimensional array into a one-dimensional array, which is often used as the first layer of a neural network or immediately after the input layer. However, there are several disadvantages to using flattening as the first layer or immediately after the input layer, including:\n",
    "\n",
    "Loss of spatial information: By flattening the input, all spatial information is lost, which can make it difficult for the network to learn the spatial relationships between the input features.\n",
    "\n",
    "Limited ability to learn complex patterns: Because flattening removes the spatial structure of the input, it limits the network's ability to learn more complex patterns in the data.\n",
    "\n",
    "Increased number of parameters: A larger number of parameters is required to learn the same number of features, which can make the network more prone to overfitting.\n",
    "\n",
    "To overcome these issues, convolutional layers can be used instead of flattening. Convolutional layers are designed to preserve the spatial structure of the input, allowing the network to learn more complex patterns in the data. Additionally, convolutional layers typically have fewer parameters, which reduces the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bef373",
   "metadata": {},
   "source": [
    "# 4\n",
    "Convolutional Neural Networks (CNNs) are a type of deep learning neural network that are specifically designed to process data with a grid-like topology, such as an image. They are particularly useful for image and video recognition, as well as natural language processing tasks.\n",
    "\n",
    "One of the key advantages of CNNs over regular fully connected neural networks (FCNs) is their ability to learn spatial hierarchies of features. CNNs are able to learn both local and global features from images, through the use of convolutional layers, pooling layers and fully connected layers. This allows them to be more robust to translation, scaling, and deformation.\n",
    "\n",
    "Another advantage is that CNNs have a lot fewer parameters than FCNs. The convolution operation effectively shares parameters across all positions in the input, which can reduce the number of parameters by several orders of magnitude. This makes CNNs more computationally efficient, and less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095b830",
   "metadata": {},
   "source": [
    "# 5\n",
    "Output[i, j, k] = sum(Input[i+m, j+n, :] * Filter[m, n, :]) for all m, n\n",
    "\n",
    "Where:\n",
    "\n",
    "Output[i, j, k] is the value of the k-th feature map at position (i, j)\n",
    "Input is the input data to the convolutional layer\n",
    "Filter is the set of filters used by the layer\n",
    "m, n are the spatial indices of the filter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e5d7b",
   "metadata": {},
   "source": [
    "# 6\n",
    "\n",
    "Without padding, the output feature map size would be:\n",
    "\n",
    "Output size = [(Input size - Kernel size) / Stride] + 1\n",
    "\n",
    "Output size = [(9-2) / 2] + 1 = 4 x 4\n",
    "\n",
    "In this case, padding is not required to calculate the output because the output size is smaller than the input size.\n",
    "\n",
    "Alternatively, it can be computed as\n",
    "\n",
    "Output size = (Input size - kernel size + 2*padding)/stride +1\n",
    "\n",
    "Output size = (9-2)/2 +1 = 4 x 4\n",
    "\n",
    "With padding, the output feature map size would be the same as without padding, but the calculation of each element of the output feature map would be different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ccdef7",
   "metadata": {},
   "source": [
    "# 7\n",
    "Data augmentation: This technique involves generating additional training data by applying various transformations to the existing training data, such as rotation, scaling, and flipping. This can help to increase the diversity of the training data and reduce overfitting.\n",
    "\n",
    "Regularization: This technique involves adding a penalty term to the loss function to discourage the model from having too many parameters. Common forms of regularization include L1, L2 regularization, and dropout.\n",
    "\n",
    "Early stopping: This technique involves monitoring the performance of the model on a validation set during training, and stopping the training when the performance on the validation set starts to decrease. This can help to prevent overfitting.\n",
    "\n",
    "Hyperparameter tuning: This technique involves systematically varying the hyperparameters of the model, such as the learning rate, batch size, and the number of layers, and selecting the best combination of hyperparameters based on the performance on a validation set.\n",
    "\n",
    "Model Ensemble: This technique involves training multiple models and combining their predictions to improve the overall performance.\n",
    "\n",
    "Transfer learning: This technique involves using a pre-trained model as a starting point for a new problem, rather than training a model from scratch. This can be useful when the new problem is similar to a problem that has already been solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f68d3",
   "metadata": {},
   "source": [
    "# 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c959929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3796a1df",
   "metadata": {},
   "source": [
    " a)After Performing EDA came to know that length of the labels and images are same and there are 10 uniqure breeds exsisitng in dataset,i used 1000 images for expermenting ,then \n",
    " Take image filename as input.\n",
    "Uses TensorFlow to read a file and save it to a variable, image.\n",
    "Turn the image into Tensors.\n",
    "Resize the image to the shape (224, 224).\n",
    "Return the modified image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83a779",
   "metadata": {},
   "source": [
    "b)With a limited number of training images per class,  \n",
    "transformations to the images, such as rotation, scaling, and flipping, to artificially increase the number of training examples.\n",
    " where a pre-trained model is fine-tuned on the target dataset, and using a combination of multiple models (ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd85888",
   "metadata": {},
   "source": [
    "#c\n",
    "To iterate and preserve the results of each experiment, used TensorBoard, a visualization tool that allows you to track and compare the performance of different models. TensorBoard can be used to track metrics such as training and validation loss,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
